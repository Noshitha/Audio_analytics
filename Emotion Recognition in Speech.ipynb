{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip install librosa pandas numpy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions mapping\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "observed_emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list all .wav files in a directory and subdirectories\n",
    "def list_wav_files(directory):\n",
    "    wav_files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".wav\"):\n",
    "                wav_files.append(os.path.join(root, filename))\n",
    "    return wav_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING & FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from audio file\n",
    "def extract_features(file_path, mfcc=True, chroma=True, mel=True, pitch=True, energy=True):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    features = []\n",
    "    # Acoustic Features\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0)\n",
    "        features.extend(mfccs)\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(audio))\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0)\n",
    "        features.extend(chroma)\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T, axis=0)\n",
    "        features.extend(mel)\n",
    "        \n",
    "    # Temporal Features    \n",
    "    if pitch:\n",
    "        pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr)\n",
    "        pitch = np.mean(pitches[pitches > 0])\n",
    "        features.append(pitch)\n",
    "    if energy:\n",
    "        energy = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n",
    "        features.append(energy)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data and extract features for each audio file\n",
    "def load_data_and_extract_features(file_paths):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file_path in file_paths:\n",
    "        parts = os.path.basename(file_path).split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], None)\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        features = extract_features(file_path)\n",
    "        data.append(features)\n",
    "        labels.append(emotion)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2880\n",
      "Number of labels: 2880\n"
     ]
    }
   ],
   "source": [
    "audio_path = '/Users/noshitha/Downloads/AUDIO DATA'\n",
    "wav_files = list_wav_files(audio_path)\n",
    "data, labels = load_data_and_extract_features(wav_files)\n",
    "\n",
    "# Print the number of samples and labels\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad features to ensure consistent feature lengths\n",
    "def pad_features(features, max_len=180):\n",
    "    padded_features = np.zeros((len(features), max_len))\n",
    "    for i, feature in enumerate(features):\n",
    "        if len(feature) > max_len:\n",
    "            padded_features[i, :max_len] = feature[:max_len]\n",
    "        else:\n",
    "            padded_features[i, :len(feature)] = feature\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9201388888888888\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        83\n",
      "           1       0.92      1.00      0.96        72\n",
      "           2       0.92      0.92      0.92        77\n",
      "           3       0.91      0.89      0.90        72\n",
      "           4       0.88      0.84      0.86        87\n",
      "           5       1.00      0.85      0.92        41\n",
      "           6       0.93      0.93      0.93        82\n",
      "           7       0.89      1.00      0.94        62\n",
      "\n",
      "    accuracy                           0.92       576\n",
      "   macro avg       0.93      0.92      0.92       576\n",
      "weighted avg       0.92      0.92      0.92       576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pad features to ensure consistent lengths\n",
    "padded_data = pad_features(data)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_data, encoded_labels, test_size=0.2, random_state=9)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "with open('rf_emotion_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL TIME PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n",
      "Predicted Emotion (Random Forest): happy\n"
     ]
    }
   ],
   "source": [
    "# Function to pad features to ensure consistent feature lengths\n",
    "def pad_features_real_time(features, max_len=180):\n",
    "    padded_features = np.zeros((1, max_len))\n",
    "    if len(features) > max_len:\n",
    "        padded_features[0, :max_len] = features[:max_len]\n",
    "    else:\n",
    "        padded_features[0, :len(features)] = features\n",
    "    return padded_features\n",
    "\n",
    "# Function to load the trained model and make predictions\n",
    "def predict_emotion(file_path, model_path='rf_emotion_classifier.pkl'):\n",
    "    features = extract_features(file_path)\n",
    "    padded_features = pad_features_real_time(features)\n",
    "    \n",
    "    # Load the trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    # Predict the emotion\n",
    "    prediction = model.predict(padded_features)\n",
    "    \n",
    "    # Convert prediction to emotion label\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(observed_emotions)\n",
    "    emotion = label_encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return emotion[0]\n",
    "\n",
    "# Real-time audio processing\n",
    "def process_real_time_audio():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    RECORD_SECONDS = 5\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    return WAVE_OUTPUT_FILENAME\n",
    "\n",
    "# Capture and process audio in real-time\n",
    "audio_file = process_real_time_audio()\n",
    "rf_emotion = predict_emotion(audio_file)\n",
    "print(f\"Predicted Emotion (Random Forest): {rf_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
