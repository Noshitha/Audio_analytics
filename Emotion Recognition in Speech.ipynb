{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip install librosa pandas numpy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions mapping\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "observed_emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list all .wav files in a directory and subdirectories\n",
    "def list_wav_files(directory):\n",
    "    wav_files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".wav\"):\n",
    "                wav_files.append(os.path.join(root, filename))\n",
    "    return wav_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING & FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from audio file\n",
    "def extract_features(file_path, mfcc=True, chroma=True, mel=True, pitch=True, energy=True):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    features = []\n",
    "    # Acoustic Features\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0)\n",
    "        features.extend(mfccs)\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(audio))\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0)\n",
    "        features.extend(chroma)\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T, axis=0)\n",
    "        features.extend(mel)\n",
    "        \n",
    "    # Temporal Features    \n",
    "    if pitch:\n",
    "        pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr)\n",
    "        pitch = np.mean(pitches[pitches > 0])\n",
    "        features.append(pitch)\n",
    "    if energy:\n",
    "        energy = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n",
    "        features.append(energy)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data and extract features for each audio file\n",
    "def load_data_and_extract_features(file_paths):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file_path in file_paths:\n",
    "        parts = os.path.basename(file_path).split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], None)\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        features = extract_features(file_path)\n",
    "        data.append(features)\n",
    "        labels.append(emotion)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2880\n",
      "Number of labels: 2880\n"
     ]
    }
   ],
   "source": [
    "audio_path = '/Users/noshitha/Downloads/AUDIO DATA'\n",
    "wav_files = list_wav_files(audio_path)\n",
    "data, labels = load_data_and_extract_features(wav_files)\n",
    "\n",
    "# Print the number of samples and labels\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad features to ensure consistent feature lengths\n",
    "def pad_features(features, max_len=180):\n",
    "    padded_features = np.zeros((len(features), max_len))\n",
    "    for i, feature in enumerate(features):\n",
    "        if len(feature) > max_len:\n",
    "            padded_features[i, :max_len] = feature[:max_len]\n",
    "        else:\n",
    "            padded_features[i, :len(feature)] = feature\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9201388888888888\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        83\n",
      "           1       0.92      1.00      0.96        72\n",
      "           2       0.92      0.92      0.92        77\n",
      "           3       0.91      0.89      0.90        72\n",
      "           4       0.88      0.84      0.86        87\n",
      "           5       1.00      0.85      0.92        41\n",
      "           6       0.93      0.93      0.93        82\n",
      "           7       0.89      1.00      0.94        62\n",
      "\n",
      "    accuracy                           0.92       576\n",
      "   macro avg       0.93      0.92      0.92       576\n",
      "weighted avg       0.92      0.92      0.92       576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pad features to ensure consistent lengths\n",
    "padded_data = pad_features(data)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_data, encoded_labels, test_size=0.2, random_state=9)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "with open('rf_emotion_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL TIME PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n",
      "Predicted Emotion (Random Forest): happy\n"
     ]
    }
   ],
   "source": [
    "# Function to pad features to ensure consistent feature lengths\n",
    "def pad_features_real_time(features, max_len=180):\n",
    "    padded_features = np.zeros((1, max_len))\n",
    "    if len(features) > max_len:\n",
    "        padded_features[0, :max_len] = features[:max_len]\n",
    "    else:\n",
    "        padded_features[0, :len(features)] = features\n",
    "    return padded_features\n",
    "\n",
    "# Function to load the trained model and make predictions\n",
    "def predict_emotion(file_path, model_path='rf_emotion_classifier.pkl'):\n",
    "    features = extract_features(file_path)\n",
    "    padded_features = pad_features_real_time(features)\n",
    "    \n",
    "    # Load the trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    # Predict the emotion\n",
    "    prediction = model.predict(padded_features)\n",
    "    \n",
    "    # Convert prediction to emotion label\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(observed_emotions)\n",
    "    emotion = label_encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return emotion[0]\n",
    "\n",
    "# Real-time audio processing\n",
    "def process_real_time_audio():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    RECORD_SECONDS = 5\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    return WAVE_OUTPUT_FILENAME\n",
    "\n",
    "# Capture and process audio in real-time\n",
    "audio_file = process_real_time_audio()\n",
    "rf_emotion = predict_emotion(audio_file)\n",
    "print(f\"Predicted Emotion (Random Forest): {rf_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (fsevents)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 15, in <module>\n",
      "    from ipykernel import kernelapp as app\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/__init__.py\", line 7, in <module>\n",
      "    from .connect import *\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/connect.py\", line 12, in <module>\n",
      "    import jupyter_client\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/jupyter_client/__init__.py\", line 3, in <module>\n",
      "    from .asynchronous import AsyncKernelClient\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/jupyter_client/asynchronous/__init__.py\", line 1, in <module>\n",
      "    from .client import AsyncKernelClient  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/jupyter_client/asynchronous/client.py\", line 11, in <module>\n",
      "    from ..channels import AsyncZMQSocketChannel, HBChannel\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/jupyter_client/channels.py\", line 12, in <module>\n",
      "    from jupyter_core.utils import ensure_async\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/jupyter_core/utils/__init__.py\", line 13, in <module>\n",
      "    from pathlib import Path\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pathlib.py\", line 10, in <module>\n",
      "    from collections import Sequence\n",
      "ImportError: cannot import name 'Sequence' from 'collections' (/opt/anaconda3/lib/python3.11/collections/__init__.py)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "model_path = '/Users/noshitha/Desktop/Github/Audio_analytics/rf_emotion_classifier.pkl'\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "with open(model_path, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "observed_emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "def extract_features(file_path, mfcc=True, chroma=True, mel=True, pitch=True, energy=True):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    features = []\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0)\n",
    "        features.extend(mfccs)\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(audio))\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0)\n",
    "        features.extend(chroma)\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T, axis=0)\n",
    "        features.extend(mel)\n",
    "    if pitch:\n",
    "        pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr)\n",
    "        pitch = np.mean(pitches[pitches > 0])\n",
    "        features.append(pitch)\n",
    "    if energy:\n",
    "        energy = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n",
    "        features.append(energy)\n",
    "    return features\n",
    "\n",
    "def pad_features(features, max_len=180):\n",
    "    padded_features = np.zeros((1, max_len))\n",
    "    if len(features) > max_len:\n",
    "        padded_features[0, :max_len] = features[:max_len]\n",
    "    else:\n",
    "        padded_features[0, :len(features)] = features\n",
    "    return padded_features\n",
    "\n",
    "def predict_emotion(file_path):\n",
    "    features = extract_features(file_path)\n",
    "    padded_features = pad_features(features)\n",
    "    prediction = model.predict(padded_features)\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(observed_emotions)\n",
    "    emotion = label_encoder.inverse_transform(prediction)\n",
    "    return emotion[0]\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    audio_file = request.files['file']\n",
    "    file_path = 'temp.wav'\n",
    "    audio_file.save(file_path)\n",
    "    emotion = predict_emotion(file_path)\n",
    "    return jsonify({'emotion': emotion})\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='127.0.0.1', port=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52394) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda update jupyter_core jupyter_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52407) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter_client in /opt/anaconda3/lib/python3.11/site-packages (8.6.2)\n",
      "Requirement already satisfied: jupyter_core in /opt/anaconda3/lib/python3.11/site-packages (5.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter_client) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter_client) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter_client) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter_client) (5.7.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter_core) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter_client) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade jupyter_client jupyter_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
